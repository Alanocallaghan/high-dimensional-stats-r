---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-high-dimensional-regression.md in _episodes_rmd/
title: "High dimensional regression"
teaching: 0
exercises: 0
questions:
- "How can we apply regression methods in a high-dimensional setting?"
- "How can we control for the fact that we do many tests?"
- "How can we benefit from the fact that we have many variables?"
- "How can we find a good subset of variables to use for regression?"
objectives:
- "Perform and critically analyse high dimensional regression."
- "Perform multiple testing adjustment."
- "Perform and critically analyse penalised regression."
keypoints:
- "Multiple testing correction can enable us to account for many null hypothesis
    significance tests while retaining power."
- "Sharing information between features can increase power and reduce false 
    positives."
- "Modelling features together can help to identify a subset of features
    that contribute to the outcome."
math: yes
---





Linear regression with one predictor variable $x$ comprises the following 
equation

$$
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$


where $\epsilon_i$ is the *noise*, or the variation in $y$ that isn't explained
by the relationship we're modelling. We assume this noise follows a normal
distribution, that is:

$$
    \epsilon_i \sim N(0, \sigma^2)
$$

We can also write this using linear algebra (matrices and vectors) as follows: 

$$
    y = X\beta + \epsilon
$$

Another way of saying this is that y follows a normal distribution with

$$
    y \sim N(X\beta, \sigma^2)
$$



> ## Exercise
> Launch `shinystats::regressionApp` and adjust the parameters.
> 
> How does the degree of noise affect the level of certainty in the fitted
> trend? What about the number of observations?
> 
> > ## Solution
> > ??? What do I put here...
> {: .solution}
{: .challenge}


Do linear regression on each feature.


~~~
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("qvalue")
    library("minfi")
    library("here")
    library("FlowSorted.Blood.EPIC")
    library("IlluminaHumanMethylationEPICmanifest")
    library("IlluminaHumanMethylationEPICanno.ilm10b4.hg19")
    library("ExperimentHub")
    library("here")
    library("broom")
})

if (!file.exists(here("data/FlowSorted_Blood_EPIC.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/FlowSorted_Blood_EPIC.rds"))

lim <- norm
y <- lim$Age
X <- getM(lim)

dfs <- lapply(1:nrow(lim),
    function(i) {
        # cat(i, "/", nrow(X), "\n")
        df <- tidy(lm(X[i, ] ~ y))[2, ]
        df$term <- rownames(X)[[i]]
        df
    }
)
df_all <- do.call(rbind, dfs)
plot(df_all$estimate, -log10(df_all$p.value))
~~~
{: .language-r}

<img src="../fig/rmd-02-unnamed-chunk-2-1.png" title="plot of chunk unnamed-chunk-2" alt="plot of chunk unnamed-chunk-2" width="612" style="display: block; margin: auto;" />


```
## age - strong comparison
design <- model.matrix(~lim$Age)
colnames(design) <- c("intercept", "age")
fit <- lmFit(getM(lim), design = design)
fit <- eBayes(fit)
tt1 <- topTable(fit, coef = 2, number = nrow(fit))

plot(tt1$logFC, -log10(tt1$P.Value))
q <- qvalue(tt1$P.Value)
hist(q)
```


> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> 
> > ## Solution
> > 
> > ~~~
> > library("Seurat")
> > ~~~
> > {: .language-r}
> > 
> > 
> > 
> > ~~~
> > Error in library("Seurat"): there is no package called 'Seurat'
> > ~~~
> > {: .error}
> > ??? What do I put here...
> {: .solution}
{: .challenge}




~~~
## challenge 2:
## Create a set of normal predictors, X
## then create a normally distributed outcome, y
## that depends on a subset of X
noise_sd <- 2
npred <- 99
frac <- 0.2
X <- replicate(npred - 1, rnorm(nobs, mean = 0, sd = 1))
~~~
{: .language-r}



~~~
Error in rnorm(nobs, mean = 0, sd = 1): invalid arguments
~~~
{: .error}



~~~
colnames(X) <- paste0("predictor_", 1:(npred-1))
~~~
{: .language-r}



~~~
Error in dimnames(x) <- dn: length of 'dimnames' [2] not equal to array extent
~~~
{: .error}



~~~
noise <- rnorm(nobs, mean = 0, sd = noise_sd)
~~~
{: .language-r}



~~~
Error in rnorm(nobs, mean = 0, sd = noise_sd): invalid arguments
~~~
{: .error}



~~~
X <- cbind(intercept = rep(1, nobs), X)
~~~
{: .language-r}



~~~
Error in eval(quote(list(...)), env): cannot coerce type 'closure' to vector of type 'double'
~~~
{: .error}



~~~
beta <- rep(0, npred)
names(beta) <- colnames(X)
ind <- as.logical(rbinom(npred, 1, frac))
beta[ind] <- rnorm(sum(ind)) + sample(c(-2, 2), sum(ind), replace=TRUE)
y <- ((X %*% beta) + noise)[, 1]
~~~
{: .language-r}



~~~
Error in X %*% beta: non-conformable arguments
~~~
{: .error}


~~~
## challenge 3: fit y on x univariate
## compare with true betas
cc <- sapply(1:ncol(X), function(i) {
    coef(lm(y ~ X[, i]))[[2]]
})
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'object' in selecting a method for function 'coef': variable lengths differ (found for 'X[, i]')
~~~
{: .error}



~~~
plot(cc, beta, pch = 19, cex = 0.5)
~~~
{: .language-r}



~~~
Error in xy.coords(x, y, xlabel, ylabel, log): 'x' and 'y' lengths differ
~~~
{: .error}



~~~
abline(0, 1)
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(v = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(X, y = y))
~~~
{: .language-r}



~~~
Warning in cbind(...): number of rows of result is not a multiple of vector
length (arg 2)
~~~
{: .warning}



~~~
int <- lm(y ~ 1, data=xy)
all <- lm(y ~ . + 0, data=xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
forward$anova
~~~
{: .language-r}



~~~
  Step Df Deviance Resid. Df Resid. Dev     AIC
1      NA       NA      4999     576502 23739.7
~~~
{: .output}



~~~
plot(coef(forward), beta[names(coef(forward))])
~~~
{: .language-r}



~~~
Warning in min(x): no non-missing arguments to min; returning Inf
~~~
{: .warning}



~~~
Warning in max(x): no non-missing arguments to max; returning -Inf
~~~
{: .warning}



~~~
Error in plot.window(...): need finite 'ylim' values
~~~
{: .error}

<img src="../fig/rmd-02-unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" width="612" style="display: block; margin: auto;" />

~~~
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}


~~~
## note about backward/both, not a challenge
all <- lm(y ~ . + 0, data=xy)
backward <- step(
    all,
    scope = formula(all),
    direction = "backward",
    trace = 0
)
backward$anova
~~~
{: .language-r}



~~~
                      Step Df    Deviance Resid. Df Resid. Dev      AIC
1                          NA          NA      4963    5626621 35203.13
2  - `201868590206_R08C01`  1    4.925358      4964    5626626 35201.14
3  - `201870610056_R04C01`  1   19.584930      4965    5626646 35199.15
4  - `201868590206_R04C01`  1   27.506376      4966    5626673 35197.18
5  - `201869680009_R07C01`  1   48.146129      4967    5626721 35195.22
6  - `201870610111_R02C01`  1   46.573247      4968    5626768 35193.26
7  - `201868590243_R03C01`  1   68.048607      4969    5626836 35191.32
8  - `201868590243_R07C01`  1  203.093459      4970    5627039 35189.50
9  - `201868590243_R06C01`  1  178.229378      4971    5627217 35187.66
10 - `201869680009_R08C01`  1  163.863091      4972    5627381 35185.81
11 - `201868590243_R04C01`  1  202.339158      4973    5627583 35183.99
12 - `201868500150_R07C01`  1  249.456845      4974    5627833 35182.21
13 - `201869680030_R01C01`  1  349.487180      4975    5628182 35180.52
14 - `201869680030_R06C01`  1  252.003787      4976    5628434 35178.74
15 - `201870610111_R05C01`  1  579.227401      4977    5629013 35177.26
16 - `201870610111_R06C01`  1  626.530027      4978    5629640 35175.81
17 - `201868590267_R07C01`  1  678.349961      4979    5630318 35174.42
18 - `201868590206_R05C01`  1  938.324254      4980    5631257 35173.25
19 - `201869680030_R02C01`  1 1282.537896      4981    5632539 35172.39
20 - `201868590193_R06C01`  1 1378.390290      4982    5633918 35171.61
21 - `201869680009_R06C01`  1 1827.984434      4983    5635746 35171.23
~~~
{: .output}



~~~
plot(coef(backward), beta[names(coef(backward))])
~~~
{: .language-r}



~~~
Warning in min(x): no non-missing arguments to min; returning Inf
~~~
{: .warning}



~~~
Warning in max(x): no non-missing arguments to max; returning -Inf
~~~
{: .warning}



~~~
Error in plot.window(...): need finite 'ylim' values
~~~
{: .error}

<img src="../fig/rmd-02-unnamed-chunk-7-1.png" title="plot of chunk unnamed-chunk-7" alt="plot of chunk unnamed-chunk-7" width="612" style="display: block; margin: auto;" />

~~~
abline(0, 1)
abline(v = 0, lty="dashed", col = "firebrick")
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}


~~~
## Challenge 5:
## one of these...? probably lasso
library("glmnet")
ridge <- cv.glmnet(X[, -1], y, alpha = 0)
~~~
{: .language-r}



~~~
Error in glmnet(x, y, weights = weights, offset = offset, lambda = lambda, : number of observations in y (37) not equal to the number of rows of x (5000)
~~~
{: .error}



~~~
lasso <- cv.glmnet(X[, -1], y, alpha = 1)
~~~
{: .language-r}



~~~
Error in glmnet(x, y, weights = weights, offset = offset, lambda = lambda, : number of observations in y (37) not equal to the number of rows of x (5000)
~~~
{: .error}



~~~
elastic <- cv.glmnet(X[, -1], y, alpha = 0.5, intercept = FALSE)
~~~
{: .language-r}



~~~
Error in glmnet(x, y, weights = weights, offset = offset, lambda = lambda, : number of observations in y (37) not equal to the number of rows of x (5000)
~~~
{: .error}



~~~
plot(coef(lasso, s = lasso$lambda.1se)[, 1], beta)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': error in evaluating the argument 'object' in selecting a method for function 'coef': object 'lasso' not found
~~~
{: .error}



~~~
abline(0, 1)
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(v = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
plot(coef(elastic, s = elastic$lambda.1se)[, 1], beta)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': error in evaluating the argument 'object' in selecting a method for function 'coef': object 'elastic' not found
~~~
{: .error}



~~~
abline(0, 1)
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(v = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
plot(coef(ridge, s = ridge$lambda.1se)[, 1], beta)
~~~
{: .language-r}



~~~
Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'plot': error in evaluating the argument 'object' in selecting a method for function 'coef': object 'ridge' not found
~~~
{: .error}



~~~
abline(0, 1)
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(v = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}



~~~
abline(h = 0, lty="dashed", col = "firebrick")
~~~
{: .language-r}



~~~
Error in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): plot.new has not been called yet
~~~
{: .error}










~~~
x <- t(getM(norm))
y <- as.numeric(factor(norm$smoker)) - 1

fit <- cv.glmnet(x = x, y = y, family="binomial")
~~~
{: .language-r}



~~~
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
Warning in lognet(xd, is.sparse, ix, jx, y, weights, offset, alpha, nobs, : one
multinomial or binomial class has fewer than 8 observations; dangerous ground
~~~
{: .warning}



~~~
c <- coef(fit, s = fit$lambda.1se)
c[c[, 1] != 0, 1]
~~~
{: .language-r}



~~~
[1] -1.455287
~~~
{: .output}



~~~
y <- norm$Age
fit <- cv.glmnet(x = x, y = y)

c <- coef(fit, s = fit$lambda.1se)
coef <- c[c[, 1] != 0, 1]

plot(y, x[, names(which.max(coef[-1]))])
~~~
{: .language-r}

<img src="../fig/rmd-02-unnamed-chunk-9-1.png" title="plot of chunk unnamed-chunk-9" alt="plot of chunk unnamed-chunk-9" width="612" style="display: block; margin: auto;" />







{% include links.md %}

