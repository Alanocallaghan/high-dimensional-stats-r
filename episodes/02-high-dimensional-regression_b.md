---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-high-dimensional-regression_b.md in _episodes_rmd/
title: "Stepwise feature selection for regression"
teaching: 0
exercises: 0
questions:
- "Why would we want to find a subset of features
  that are associated with an outcome?"
- "How can we iteratively find a good subset of our features
  variables to use for regression?"
- "What are some risks and downsides of iterative feature
  selection?"
objectives:
- "Understand multiple regression in a biomedical context."
- "Understand how to fit a stepwise regression model."
keypoints:
- "Sets of features can be more predictive and provide
  a better explanation than a single feature alone."
- "Stepwise regression allows us to find a set of features that
  are associated with an outcome (eg, age)."
- "Stepwise regression will tend to retain only one
  feature out of many that are correlated."
- "Stepwise regression is not very efficient."
math: yes
---





In the previous 

Another way of modelling these data is to model age as 

$$
    y_j = \beta_0 + \beta_1 X_1 + \dots \beta_p X_p + \epsilon_j
$$



~~~
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("minfi")
    library("here")
    library("broom")
})

if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/methylation.rds"))

lim <- norm
y <- lim$Age
X <- getM(lim)
~~~
{: .language-r}

However when the number of predictors is greater than the number of samples
(basically always true in genetics) it isn't possible to include everything!

There are some techniques that you can use to find a set of predictors!

- screening (correlation etc): bad, don't do
- screening (variance): not necessarily bad if the screening variable is sensible
- forward/reverse/best subset selection


~~~
if (!file.exists(here("data/synthetic.rds"))) {
    source(here("data/synthetic.R"))
}
synthetic <- readRDS(here("data/synthetic.rds"))
~~~
{: .language-r}





~~~
X <- assay(synthetic)
y <- synthetic$age
beta <- rowData(synthetic)$true_beta
names(beta) <- rownames(synthetic)
## challenge 3: fit y on x univariate
## compare with true betas
cc <- sapply(seq_len(nrow(X)), function(i) {
    coef(lm(y ~ X[i, ]))[[2]]
})
plot(cc, beta, pch = 19, cex = 0.5)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-4-1.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" width="612" style="display: block; margin: auto;" />




> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> 
> > ## Solution
> > 
> > 
> {: .solution}
{: .challenge}




~~~
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(t(X), y = y))
int <- lm(y ~ 1, data = xy)
all <- lm(y ~ . + 0, data = xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
forward$anova
~~~
{: .language-r}



~~~
           Step Df     Deviance Resid. Df   Resid. Dev         AIC
1               NA           NA        99 4333.1600000  378.888216
2  + feature_85 -1 1.028336e+03        98 3304.8241187  353.796835
3  + feature_42 -1 6.926908e+02        97 2612.1333138  332.275234
4  + feature_71 -1 3.911962e+02        96 2220.9370924  318.051431
5  + feature_73 -1 3.649750e+02        95 1855.9621021  302.098831
6  + feature_68 -1 2.361304e+02        94 1619.8317013  290.490735
7  + feature_11 -1 2.435455e+02        93 1376.2862056  276.197381
8  + feature_80 -1 1.860074e+02        92 1190.2787904  263.677265
9  + feature_53 -1 2.420905e+02        91  948.1883287  242.938296
10 + feature_27 -1 1.293940e+02        90  818.7943747  230.266280
11 + feature_78 -1 1.586049e+02        89  660.1895069  210.735674
12 + feature_47 -1 9.042080e+01        88  569.7687089  198.006032
13 + feature_49 -1 1.102715e+02        87  459.4972205  178.496271
14 + feature_79 -1 1.210680e+02        86  338.4292284  149.914481
15  + feature_8 -1 6.775572e+01        85  270.6735048  129.574313
16  + feature_3 -1 4.128491e+01        84  229.3885994  115.024732
17 + feature_52 -1 4.729426e+01        83  182.0943417   93.935473
18 + feature_37 -1 2.965331e+01        82  152.4410332   78.160767
19  + feature_5 -1 2.525392e+01        81  127.1871085   62.048911
20 + feature_22 -1 2.113041e+01        80  106.0567016   45.880369
21 + feature_75 -1 2.092786e+01        79   85.1288452   25.899575
22 + feature_66 -1 1.215782e+01        78   72.9710222   12.489222
23 + feature_69 -1 4.018001e+00        77   68.9530208    8.825523
24 + feature_48 -1 3.455798e+00        76   65.4972233    5.683756
25 + feature_39 -1 3.450375e+00        75   62.0468482    2.271953
26 + feature_40 -1 3.223951e+00        74   58.8228970   -1.063900
27 + feature_70 -1 2.195016e+00        73   56.6278811   -2.866872
28 + feature_10 -1 2.807004e+00        72   53.8208769   -5.950875
29 + feature_46 -1 2.290185e+00        71   51.5306921   -8.299259
30 + feature_32 -1 2.126589e+00        70   49.4041028  -10.513671
31 + feature_20 -1 2.000940e+00        69   47.4031633  -12.648122
32 + feature_62 -1 1.693610e+00        68   45.7095535  -14.286286
33 + feature_90 -1 1.337655e+00        67   44.3718987  -15.256383
34 + feature_26 -1 1.303126e+00        66   43.0687729  -16.237198
35 + feature_17 -1 1.157893e+00        65   41.9108803  -16.962472
36 + feature_57 -1 9.811325e-01        64   40.9297478  -17.331306
37 + feature_72 -1 1.228644e+00        63   39.7011035  -18.379120
38 + feature_58 -1 9.836268e-01        62   38.7174767  -18.887909
39 + feature_41 -1 7.907204e-01        61   37.9267564  -18.951335
40 + feature_35 -1 8.525168e-01        60   37.0742396  -19.224781
41  + feature_4 -1 9.361080e-01        59   36.1381315  -19.782160
42 + feature_61 -1 1.033799e+00        58   35.1043325  -20.684563
43 + feature_29 -1 1.064693e+00        57   34.0396391  -21.764448
44 + feature_82 -1 1.049886e+00        56   32.9897536  -22.897317
45 + feature_96 -1 8.570440e-01        55   32.1327096  -23.529568
46 + feature_59 -1 1.032030e+00        54   31.1006798  -24.794051
47 + feature_60 -1 7.464458e-01        53   30.3542340  -25.223417
48 + feature_56 -1 1.015462e+00        52   29.3387721  -26.626026
49  + feature_9 -1 1.456190e+00        51   27.8825817  -29.716800
50 + feature_51 -1 1.344247e+00        50   26.5383348  -32.657990
51 + feature_38 -1 9.541754e-01        49   25.5841594  -34.319680
52  + feature_6 -1 6.217188e-01        48   24.9624406  -34.779787
53 + feature_12 -1 7.868805e-01        47   24.1755601  -35.982798
54 + feature_54 -1 8.206339e-01        46   23.3549262  -37.436225
55 + feature_43 -1 7.519730e-01        45   22.6029532  -38.708962
56 + feature_33 -1 5.711964e-01        44   22.0317568  -39.268528
57 + feature_94 -1 6.858773e-01        43   21.3458795  -40.431146
58  + feature_1 -1 7.494494e-01        42   20.5964301  -42.005242
59 + feature_86 -1 8.921857e-01        41   19.7042444  -44.433612
60 + feature_98 -1 8.799120e-01        40   18.8243325  -47.001987
61  + feature_2 -1 6.549209e-01        39   18.1694116  -48.543069
62 + feature_93 -1 5.750110e-01        38   17.5944006  -49.758948
63 + feature_95 -1 1.210603e+00        37   16.3837972  -54.887732
64 + feature_63 -1 5.698219e-01        36   15.8139752  -56.427613
65 + feature_64 -1 5.650012e-01        35   15.2489741  -58.065796
66 + feature_87 -1 5.856977e-01        34   14.6632764  -59.982402
67 + feature_28 -1 8.180963e-01        33   13.8451801  -63.723302
68 + feature_84 -1 5.135598e-01        32   13.3316204  -65.503150
69 + feature_13 -1 8.081770e-01        31   12.5234434  -69.756783
70 + feature_45 -1 7.508366e-01        30   11.7726068  -73.939481
71 + feature_15 -1 3.119210e-01        29   11.4606858  -74.624764
72 + feature_76 -1 5.103656e-01        28   10.9503201  -77.180150
73 + feature_36 -1 4.973008e-01        27   10.4530193  -79.827932
74 + feature_65 -1 5.898253e-01        26    9.8631941  -83.636013
75 + feature_24 -1 5.172608e-01        25    9.3459333  -87.022888
76 + feature_25 -1 4.138561e-01        24    8.9320772  -89.552121
77 + feature_55 -1 5.339533e-01        23    8.3981239  -93.716185
78 + feature_31 -1 5.456057e-01        22    7.8525182  -98.433591
79 + feature_81 -1 4.167961e-01        21    7.4357221 -101.887449
80 + feature_23 -1 9.533076e-01        20    6.4824145 -113.607714
81 + feature_88 -1 6.044118e-01        19    5.8780027 -121.395315
82 + feature_44 -1 3.905169e-01        18    5.4874859 -126.269998
83 + feature_91 -1 7.298541e-01        17    4.7576318 -138.542017
84 + feature_67 -1 9.462635e-01        16    3.8113683 -158.718193
85 + feature_50 -1 4.975835e-01        15    3.3137848 -170.707922
86 + feature_92 -1 3.295693e-01        14    2.9842154 -179.183331
87 + feature_77 -1 2.585754e-01        13    2.7256400 -186.246691
88 + feature_30 -1 2.019542e-01        12    2.5236859 -191.944971
89 + feature_89 -1 2.200505e-01        11    2.3036354 -199.068172
90 + feature_18 -1 9.093038e-02        10    2.2127050 -201.095444
91 + feature_97 -1 3.116310e-01         9    1.9010740 -214.275121
92 + feature_19 -1 5.607336e-01         8    1.3403404 -247.224655
93  + feature_7 -1 6.159464e-01         7    0.7243940 -306.758996
94 + feature_16 -1 2.353195e-01         6    0.4890746 -344.041046
95 + feature_14 -1 1.529795e-01         5    0.3360951 -379.553140
96 + feature_34 -1 9.845631e-02         4    0.2376388 -412.217377
97 + feature_21 -1 3.938666e-02         3    0.1982521 -428.338600
98 + feature_83 -1 2.335897e-02         2    0.1748931 -438.875037
99 + feature_74 -1 7.024829e-02         1    0.1046448 -488.235336
~~~
{: .output}



~~~
plot(coef(forward), beta[names(coef(forward))])
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" width="612" style="display: block; margin: auto;" />


~~~
## note about backward/both, not a challenge
all <- lm(y ~ . + 0, data = xy)
backward <- step(
    all,
    scope = formula(all),
    direction = "backward",
    trace = 0
)
backward$anova
~~~
{: .language-r}



~~~
          Step Df     Deviance Resid. Df Resid. Dev       AIC
1              NA           NA         1  0.1046448 -488.2353
2 - feature_28  1 8.260600e-05         2  0.1047274 -490.1564
3 - feature_75  1 8.706954e-06         3  0.1047361 -492.1481
4 - feature_16  1 9.240104e-04         4  0.1056602 -493.2698
~~~
{: .output}



~~~
plot(coef(backward), beta[names(coef(backward))])
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" width="612" style="display: block; margin: auto;" />


{% include links.md %}
