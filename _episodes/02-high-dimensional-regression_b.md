---
# Please do not edit this file directly; it is auto generated.
# Instead, please edit 02-high-dimensional-regression_b.md in _episodes_rmd/
title: "Stepwise feature selection for regression"
teaching: 0
exercises: 0
questions:
- "Why would we want to find a subset of features
  that are associated with an outcome?"
- "How can we iteratively find a good subset of our features
  variables to use for regression?"
- "What are some risks and downsides of iterative feature
  selection?"
objectives:
- "Understand multiple regression in a biomedical context."
- "Understand how to fit a stepwise regression model."
keypoints:
- "Sets of features can be more predictive and provide
  a better explanation than a single feature alone."
- "Stepwise regression allows us to find a set of features that
  are associated with an outcome (eg, age)."
- "Stepwise regression will tend to retain only one
  feature out of many that are correlated."
- "Stepwise regression is not very efficient."
math: yes
---





In the previous 

Another way of modelling these data is to model age as 

$$
    y_j = \beta_0 + \beta_1 X_1 + \dots \beta_p X_p + \epsilon_j
$$



~~~
suppressPackageStartupMessages({
    library("glmnet")
    library("limma")
    library("minfi")
    library("here")
    library("broom")
})

if (!file.exists(here("data/methylation.rds"))) {
    source(here("data/methylation.R"))
}
norm <- readRDS(here("data/methylation.rds"))

lim <- norm
y <- lim$Age
X <- getM(lim)
~~~
{: .language-r}

However when the number of predictors is greater than the number of samples
(basically always true in genetics) it isn't possible to include everything!

There are some techniques that you can use to find a set of predictors!

- screening (correlation etc): bad, don't do
- screening (variance): not necessarily bad if the screening variable is sensible
- forward/reverse/best subset selection


~~~
if (!file.exists(here("data/synthetic.rds"))) {
    source(here("data/synthetic.R"))
}
synthetic <- readRDS(here("data/synthetic.rds"))
~~~
{: .language-r}





~~~
X <- assay(synthetic)
y <- synthetic$age
beta <- rowData(synthetic)$true_beta
names(beta) <- rownames(synthetic)
## challenge 3: fit y on x univariate
## compare with true betas
cc <- sapply(seq_len(nrow(X)), function(i) {
    coef(lm(y ~ X[i, ]))[[2]]
})
plot(cc, beta, pch = 19, cex = 0.5)
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-4-1.png" title="plot of chunk unnamed-chunk-4" alt="plot of chunk unnamed-chunk-4" width="612" style="display: block; margin: auto;" />




> ## Exercise
> Perform forward subset selection on the methylation data.
> 
> 
> > ## Solution
> > 
> > 
> {: .solution}
{: .challenge}




~~~
## challenge 4: forward selection
## compare with true betas
xy <- as.data.frame(cbind(t(X), y = y))
int <- lm(y ~ 1, data = xy)
all <- lm(y ~ . + 0, data = xy)
forward <- step(
    int,
    scope = list(upper = formula(all), lower = formula(int)),
    direction = "forward",
    trace = 0
)
forward$anova
~~~
{: .language-r}



~~~
           Step Df     Deviance Resid. Df  Resid. Dev          AIC
1               NA           NA        99 2105.640000  306.7204552
2  + feature_82 -1 516.92057297        98 1588.719427  280.5513393
3  + feature_65 -1 232.02228329        97 1356.697144  266.7638268
4  + feature_85 -1 176.28577555        96 1180.411368  254.8448088
5  + feature_67 -1 222.46376697        95  957.947601  235.9622894
6  + feature_41 -1 160.34311051        94  797.604491  219.6442663
7  + feature_79 -1 118.27262078        93  679.331870  205.5939585
8  + feature_53 -1 107.15171338        92  572.180157  190.4283715
9  + feature_15 -1  85.12255378        91  487.057603  176.3212211
10 + feature_80 -1  80.57098228        90  406.486620  160.2380829
11 + feature_20 -1 106.02446460        89  300.462156  132.0151623
12 + feature_60 -1  60.40041885        88  240.061737  111.5725942
13 + feature_75 -1  34.07551896        87  205.986218   98.2639078
14 + feature_70 -1  40.58319253        86  165.403026   78.3214889
15 + feature_96 -1  28.81056744        85  136.592458   61.1831548
16 + feature_40 -1  18.29451612        84  118.297942   48.8036188
17  + feature_8 -1  13.69855596        83  104.599386   38.4967496
18 + feature_25 -1  12.24367368        82   92.355712   28.0477374
19 + feature_28 -1  10.04089305        81   82.314819   18.5380970
20 + feature_45 -1   7.10689760        80   75.207922   11.5086381
21 + feature_94 -1   4.32186765        79   70.886054    7.5903529
22 + feature_33 -1   4.01956386        78   66.866490    3.7527761
23 + feature_30 -1   2.94609990        77   63.920390    1.2468221
24 + feature_56 -1   2.35991455        76   61.560476   -0.5150149
25 + feature_93 -1   2.32558498        75   59.234891   -2.3659447
26  + feature_7 -1   2.99649163        74   56.238399   -5.5570404
27 + feature_73 -1   2.38044893        73   53.857950   -7.8820158
28 + feature_71 -1   1.54927887        72   52.308671   -8.8008029
29 + feature_84 -1   1.92115851        71   50.387513  -10.5426804
30 + feature_97 -1   1.56594945        70   48.821563  -11.6998099
31 + feature_16 -1   1.93937871        69   46.882185  -13.7532441
32 + feature_27 -1   1.70363838        68   45.178546  -15.4547852
33  + feature_1 -1   1.54222287        67   43.636323  -16.9280277
34  + feature_4 -1   1.31392860        66   42.322395  -17.9853812
35 + feature_72 -1   1.14903591        65   41.173359  -18.7378768
36 + feature_47 -1   1.39053376        64   39.782825  -20.1734896
37 + feature_11 -1   1.48073831        63   38.302087  -21.9665805
38 + feature_49 -1   1.15581366        62   37.146273  -23.0306739
39 + feature_48 -1   1.32165808        61   35.824615  -24.6534957
40 + feature_88 -1   1.15409694        60   34.670518  -25.9280482
41 + feature_86 -1   1.15923761        59   33.511281  -27.3288072
42 + feature_24 -1   1.02293429        58   32.488346  -28.4288739
43 + feature_66 -1   1.14899282        57   31.339353  -30.0295581
44  + feature_6 -1   1.12959480        56   30.209759  -31.7005181
45 + feature_26 -1   1.22719494        55   28.982564  -33.8475790
46 + feature_51 -1   0.87348433        54   28.109079  -34.9077554
47 + feature_14 -1   1.25335149        53   26.855728  -37.4691060
48 + feature_54 -1   1.04635075        52   25.809377  -39.4432307
49 + feature_37 -1   1.57304810        51   24.236329  -43.7317481
50 + feature_52 -1   1.46499540        50   22.771334  -47.9667740
51 + feature_46 -1   0.97920928        49   21.792124  -50.3621552
52 + feature_43 -1   0.96197731        48   20.830147  -52.8768873
53 + feature_31 -1   1.06713480        47   19.763012  -56.1358066
54 + feature_12 -1   0.73111792        46   19.031894  -57.9053968
55 + feature_42 -1   0.90159427        45   18.130300  -60.7585613
56 + feature_64 -1   1.00763604        44   17.122664  -64.4767221
57  + feature_3 -1   0.88609673        43   16.236567  -67.7904250
58 + feature_34 -1   0.65216215        42   15.584405  -69.8899444
59 + feature_22 -1   0.42405417        41   15.160351  -70.6486658
60 + feature_50 -1   0.63191150        40   14.528439  -72.9062117
61 + feature_44 -1   0.39969665        39   14.128743  -73.6958968
62 + feature_68 -1   0.54335475        38   13.585388  -75.6175380
63 + feature_83 -1   0.48751391        37   13.097874  -77.2720250
64 + feature_59 -1   0.37236703        36   12.725507  -78.1561774
65 + feature_74 -1   0.43787593        35   12.287631  -79.6577026
66 + feature_92 -1   0.52017357        34   11.767458  -81.9832296
67 + feature_57 -1   0.44913602        33   11.318322  -83.8747396
68 + feature_55 -1   0.35787888        32   10.960443  -85.0877514
69 + feature_62 -1   0.25658730        31   10.703855  -85.4566193
70 + feature_90 -1   0.23156411        30   10.472291  -85.6437343
71 + feature_19 -1   0.27824662        29   10.194045  -86.3366493
72 + feature_63 -1   0.45522630        28    9.738818  -88.9050393
73  + feature_2 -1   0.23172432        27    9.507094  -89.3131924
74  + feature_9 -1   0.28547717        26    9.221617  -90.3619797
75 + feature_39 -1   0.30908176        25    8.912535  -91.7711460
76 + feature_76 -1   0.24992146        24    8.662614  -92.6153700
77 + feature_91 -1   0.31188161        23    8.350732  -94.2820980
78 + feature_10 -1   0.23393692        22    8.116795  -95.1234798
79 + feature_61 -1   0.17292132        21    7.943874  -95.2769144
80 + feature_32 -1   0.20096748        20    7.742906  -95.8393074
81 + feature_17 -1   0.30045452        19    7.442452  -97.7969846
82 + feature_81 -1   0.31186450        18    7.130587 -100.0776582
83 + feature_58 -1   0.27236069        17    6.858227 -101.9721287
84 + feature_18 -1   0.20733477        16    6.650892 -103.0419227
85 + feature_13 -1   0.29736368        15    6.353528 -105.6159910
86 + feature_36 -1   0.31778125        14    6.035747 -108.7470574
87 + feature_38 -1   0.34901395        13    5.686733 -112.7034274
88 + feature_87 -1   0.29889703        12    5.387836 -116.1026377
89 + feature_95 -1   0.49492617        11    4.892910 -123.7383014
90 + feature_29 -1   0.64152699        10    4.251383 -135.7925895
91 + feature_77 -1   0.46331577         9    3.788067 -145.3314320
92 + feature_69 -1   0.76597823         8    3.022089 -165.9221943
93 + feature_89 -1   0.29326675         7    2.728822 -174.1300157
94 + feature_78 -1   0.31624539         6    2.412577 -184.4474860
95  + feature_5 -1   0.29868470         5    2.113892 -195.6639410
96 + feature_23 -1   0.15312406         4    1.960768 -201.1834009
97 + feature_35 -1   0.04424971         3    1.916518 -201.4660092
98 + feature_21 -1   0.39179393         2    1.524724 -222.3356608
99 + feature_98 -1   0.21179730         1    1.312927 -235.2911222
~~~
{: .output}



~~~
plot(coef(forward), beta[names(coef(forward))])
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-5-1.png" title="plot of chunk unnamed-chunk-5" alt="plot of chunk unnamed-chunk-5" width="612" style="display: block; margin: auto;" />


~~~
## note about backward/both, not a challenge
all <- lm(y ~ . + 0, data = xy)
backward <- step(
    all,
    scope = formula(all),
    direction = "backward",
    trace = 0
)
backward$anova
~~~
{: .language-r}



~~~
          Step Df     Deviance Resid. Df Resid. Dev       AIC
1              NA           NA         1   1.312927 -235.2911
2  - feature_9  1 1.658365e-06         2   1.312929 -237.2910
3 - feature_61  1 1.130005e-03         3   1.314059 -239.2050
4 - feature_42  1 1.069704e-03         4   1.315128 -241.1236
5 - feature_96  1 3.490747e-03         5   1.318619 -242.8585
6 - feature_30  1 3.240255e-03         6   1.321859 -244.6131
7 - feature_31  1 4.157070e-03         7   1.326016 -246.2991
~~~
{: .output}



~~~
plot(coef(backward), beta[names(coef(backward))])
abline(0, 1)
abline(v = 0, lty = "dashed", col = "firebrick")
abline(h = 0, lty = "dashed", col = "firebrick")
~~~
{: .language-r}

<img src="../fig/rmd-02b-unnamed-chunk-6-1.png" title="plot of chunk unnamed-chunk-6" alt="plot of chunk unnamed-chunk-6" width="612" style="display: block; margin: auto;" />


{% include links.md %}
